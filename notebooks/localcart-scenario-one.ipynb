{"nbformat": 4, "cells": [{"source": "# LocalCart scenario one: dynamic data analysis and visualization\n***\n\n\n## Introduction \n\nThis dynamic data analysis scenario is divided into two parts.\n\n<img src=\"https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow.png\"></img>\n\n[Part 1](#part1): A web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. A streams flow aggregates these events and stores the aggregated data in a Compose for Redis database.\n\n[Part 2](#part2): A Node.js app monitors the Compose for Redis database and visualizes the aggregated data in a simple dashboard user interface. By the end of the notebook, you'll understand how to deploy a dashboard app to IBM Cloud to visualize streaming data, and how to simulate streaming data if you don't have a streaming data source.\n\n\nThis notebook runs on Python 2 with Spark 2.1. When running it on the IBM Cloud, ensure that the notebook is in \"edit mode\", which you can enable by clicking the pencil icon in the navigation bar.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"part1\"></a>\n\n***\n# Part 1: Capturing clickstream events for real-time analysis\n***\n\n\n<img src=\"https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow_part_1.png\"></img>\n\n\nIn this first example you will create a streams flow that ingests `login`, `add_to_basket` and `checkout` clickstream events, aggregates them according to our business needs and stores the aggregated data in a Redis database, which will be monitored by a real-time dashboard:\n\n<img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_streams_flow.png'></img>\n\n## Part 1 table of contents\n\n [1.1 Redis setup](#redis)<br>\n [1.2 Create a streams flow](#create_p1) <br>\n [1.3 Process login clickstream events](#login) <br>\n [1.4 Process add_to_cart clickstream events](#addtocart) <br>\n [1.5 Process checkout clickstream events](#checkout) <br>\n [1.6 Run the flow](#run_1)<br>\n\n\n<a id=\"redis\"></a>\n***\n\n## 1.1 Redis setup\n\nRedis is an in-memory database. It stores its data in RAM, making it a very fast way of storing and retrieving data. It provides a set of primitive data structures, but we only concern ourselves with [hashes](https://redis.io/commands#hash) for this exercise.\n\nA Redis hash is a data structure that allows several keys to be stored together. We are going to configure a Redis hash called `funnel` that contains the following output:\n\n- login_count - the number of people who logged into LocalCart\n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nThese are the outputs of the aggregation functions in our streaming pipeline. \n\n\n### 1.1.1 Collect your Redis connection information\n\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">IBM Cloud Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned **Compose for Redis** service and click on the service instance name.\n1. Open the _Service Credentials_ tab and view the credentials.\n```\n{\n  \"db_type\": \"redis\",\n  \"maps\": [],\n  \"name\": \"b...b\",\n  \"uri_cli\": \"redis-cli -h **HOSTNAME** -p **PORT** -a **PASSWORD**\",\n  \"deployment_id\": \"5...2\",\n  \"uri\": \"redis://admin:**PASSWORD**@**HOSTNAME**:**PORT**\"\n}\n```\n\nNote your `**HOSTNAME**`, `**PORT**`, `**PASSWORD**`, and `uri` information.\n\n\n### 1.1.2 Verify your redis connectivity\nYou can verify your redis connectivity information in this notebook by installing the Python Redis library with the following command:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "!pip install redis", "outputs": []}, {"source": "We import the library and connect to Redis with the following command. Replace the credential placeholders with your credentials.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "import redis\n# TODO replace **uri** with your Redis uri\nr = redis.StrictRedis.from_url(**uri**)", "outputs": []}, {"source": "We can then create a hash called `funnel` to store our real-time data to the database by using the `hset` function:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "r.hset('funnel', 'basket_count', 554);\nr.hset('funnel', 'basket_total', 951);\nr.hset('funnel', 'checkout_count', 21);\nr.hset('funnel', 'checkout_total', 5400);\nr.hset('funnel', 'login_count', 100);", "outputs": []}, {"source": "We can also use this connection to retrieve all the values from our `funnel` hash using `hgetall`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "r.hgetall('funnel')", "outputs": []}, {"source": "**Note:** \nThe Redis connection above seems to freeze in this notebook after a minute or so. In this case, you will need to restart the notebook kernel to restore it.\n<BR>\nWe can now create streams flows that store aggregated data in Redis.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"create_p1\"></a>\n***\n\n## 1.2 Create a streams flow\n\nIn IBM Watson Data Platform, do these steps:\n\n1. Select a project that you want to contain the streams flows. Note that this project must be attached to Cloud Object Storage and not Object Storage (Swift).\n1. Click the **Assets** tab and scroll to the _Streams flows_ section. (If no section with the name is displayed the selected project is not attached to Cloud Object Storage.)\n1. Click **+ New streams flow**.\n1. In the _New Streams Flow_ window, \n  1. Enter name `aggregate_for_redis`\n  1. Select an existing Streaming Analytics service or create a new one (choosing the _Lite_ plan, which is free.) \n  1. Select **Manually**. \n  1. Click **Create**.\n\nAn empty canvas is displayed, along with a list of _Source_, _Target_, _Processing and Analytics_ and _Alerts_ operators that you can choose from. Source operators load data and target operators store data.\n\n<a id=\"login\"></a>\n***\n\n## 1.3 Process login clickstream events\n\nFirst we need to collect `login` data from Message Hub and calculate the number of logins during a rolling one hour time window. The incoming `login` event payload has the following structure:\n```\n  {\n    \"customer_id\": \"13872\",\n    \"click_event_type\": \"login\",\n    \"total_price_of_basket\": \"0.0\",\n    \"total_number_of_items_in_basket\": \"0\",\n    \"total_number_of_distinct_items_in_basket\": \"0\",\n    \"event_time\": \"2017-07-11 20:10:52 UTC\"\n  }\n```\n\n\n### 1.3.1 Configure the source\n\n1. Drag a **MessageHub** source operator into the pipeline canvas.\n1. Configure the MessageHub operator:\n\t1. Add a connection to your Message Hub instance. For the \"brokers\" field, note that you should enter a comma-separated string (no spaces) of all brokers listed in your service credentials.\n\t1. Select the `login` topic.\n\t1. Click **Edit Schema** to specify the payload properties this operator will make available to operators that are connected to its output port. Since we only want to count the number of login events we only make the `customer_id` available.\n    1. Choose\n            - Attribute Name: `customer_id`\n            - Type: `Number` \n            - Path: `/customer_id` \n    1. Click **Save** and **Close**.         \n\n\nOur streams flow now has its first operator and looks like this: \n\n<img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_mh_config.png'></img>\n\n\n### 1.3.2 Set up aggregation functions\n\nStreaming data can be aggregated by applying functions such as sum, count, minimum, or maximum. The results of the aggregation can be done on the aggregation before it is written to the Redis database. Our aim is to calculate the number of people who logged into LocalCart for a sliding one-hour window.\n\nIn the streams flow canvas, do these steps:\n\n1. Drag an **Aggregation** operator from the _Processing and Analytics_ area, and then drop it on the canvas next to the Message Hub operator.\n2. Drag your mouse pointer from the output port of the Message Hub operator to the input port of the Aggregation operator to connect them.\n3. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n4. In the **Functions** area of the _Aggregation Properties_ pane, define one aggregation:\n    - Aggregation 1: count the logins\n        - Output Field Name - `login_count`\n        - Function Type - `Count`\n        \n    Note: To identify how many different customers have logged in during the rolling 1 hour time window, we would use the `CountDistinct` function and apply it to `customer_id`.\n\nOur streams flow now has two connected operators: a source operator and an aggregation operator. Hover over the arrow to review the data flow between them.\n\n<img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow_mh_a.png'></img>\n\n\n\n### 1.3.3 Configure the target\n\nNext, add a Redis target operator. In the streams flow canvas, do these steps:\n\n1. Drag a **Redis** operator from the _Target_ area, and then drop it on the canvas next to the Aggregation operator.\n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n1. Click the **Redis** operator to open its Properties pane. \n    - Add a connection to your Redis instance.\n      - Type in the `**HOST**`, `**PORT**` and `**PASSWORD**` credentials of your Compose for Redis service.\n    - In the **Key Template** field, type in `funnel`. \n1. Save the streams flow. The setup for `login` event processing is complete.\n\n  <img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow_mh_a_r.png'></img>    \n\n", "cell_type": "markdown", "metadata": {}}, {"source": "***", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"addtocart\"></a>\n## 1.4 Process add_to_cart clickstream events\n\nNext we need to collect `add_to_cart` event data from Message Hub and calculate the number of shopping baskets and their combined value during a rolling one hour time window. The incoming `add_to_cart` event payload has the following structure:\n\n```\n{\n    \"customer_id\": \"13859\",\n    \"click_event_type\": \"add_to_cart\",\n    \"product_name\": \"Oatmeal\",\n    \"product_category\": \"Food\",\n    \"product_price\": \"2.49\",\n    \"total_price_of_basket\": \"153.41\",\n    \"total_number_of_items_in_basket\": \"19\",\n    \"total_number_of_distinct_items_in_basket\": \"6\",\n    \"event_time\": \"2017-06-23 12:56:18 UTC\"\n}\n```\n\n### 1.4.1 Configure the source, aggregation function and target for add_to_cart events\n\n1. Drag another **Message Hub** source operator into the canvas.\n1. Configure the Message Hub operator by doing these steps in the Properties pane:\n\t1. Select the Message Hub connection you've created earlier.\n\t1. Select the `add_to_cart` topic.\n\t1. Click **Edit Schema** to make the customer id and cart value available to connected operators. \n    1. The message schema can be automatically detected if a producer has already generated messages for the selected topic. Click **Detect Schema** and **Show preview**.\n      > If no messsages are displayed and the schema is not populated verify that your producer is running.\n    1. Remove all attributes except `customer_id` and `total_price_of_basket`:\n      - Attribute Name: `customer_id`\n            - Type: `Number` \n            - JSON Path: `/customer_id` \n      - Attribute Name: `total_price_of_basket` \n            - Type: `Number` \n            - JSON Path: `/total_price_of_basket` \n    1. Click **Save** and **Close**.\n1. Drag an **Aggregation** operator from the **Processing and Analytics** area, and then drop it on the canvas next to the Message Hub operator.\n1. Drag your mouse pointer from the output port of the Message Hub operator to the input port of the Aggregation operator to connect them.\n1. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n1. In the **Functions** area of the _Aggregation Properties_ pane, define two aggregations:\n    - Aggregation 1: count the baskets\n        - Output Field Name - `basket_count`\n        - Function Type - `Count`\n    - Aggregation 2: Sum up basket values\n        - Output Field Name - `basket_total`\n        - Function Type - `Sum`\n        - Apply Function to - `total_price_of_basket`\n        \n1. Copy the existing **Redis** operator that's already on the canvas and paste it next to the _Aggregation_ Operator. \n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n\n Your pipeline is now configured to stream and aggregate `login` and `add_to_cart` events:\n    \n <img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow_mh_a_r_2.png'></img>    \n\n1. Save your streams flow. No errors should be reported.\n\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"checkout\"></a>\n***\n\n## 1.5 Process checkout clickstream events\n\nFirst we need to create a stream that collects `checkout` event data from a Message Hub operator and calculates the number of checkouts and their combined value during a rolling one hour time window. The incoming `checkout` event payload has the following structure:\n\n```\n{\n    \"customer_id\": \"11828\",\n    \"click_event_type\": \"checkout\",\n    \"total_price_of_basket\": \"72.80000000000001\",\n    \"total_number_of_items_in_basket\": \"20\",\n    \"total_number_of_distinct_items_in_basket\": \"5\",\n    \"session_duration\": \"440\",\n    \"event_time\": \"2017-06-23 13:09:12 UTC\"\n}\n```\n\n### 1.5.1 Set up pipeline source, aggregation function and target for checkout events\n\n1. Drag another **Message Hub** source operator into the canvas.\n1. Configure the MessageHub operator by doing these steps in the Properties pane:\n\t1. Select the Message Hub connection you created earlier.\n\t1. Select the `checkout` topic.\n\t1. Click **Edit Schema** to specify the message attributes that will be consumed. Define the following attributes (by entering them manually or customizing the auto-detected schema):\n      - Attribute Name: `customer_id` \n            - Type: `Number` \n            - Path: `/customer_id` \n      - Attribute Name: `total_price_of_basket` \n            - Type: `Number` \n            - Path: `/total_price_of_basket` \n1. Drag an **Aggregation** operator from the _Processing and Analytics_ area, and then drop it on the canvas next to the Message Hub operator.\n1. Drag your mouse pointer from the output port of the Message Hub operator to the input port of the Aggregation operator to connect them.\n1. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n1. In the **Functions** area of the _Aggregation Properties_ pane, define two aggregations:\n    - Aggregation 1: count checkouts\n        - Output Field Name - `checkout_count`\n        - Function Type - `Count`\n    - Aggregation 2: Sum basket values\n        - Output Field Name - `checkout_total`\n        - Function Type - `Sum`\n        - Apply Function to - `total_price_of_basket`\n        \n1. Copy the existing **Redis** operator that's already on the canvas and paste it next to the _Aggregation_ Operator. \n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them. The completed stream now looks as follows: <br>\n   <img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow_mh_a_r_3.png'></img>    \n\n1. Save the stream flow. No errors should be reported.\n\n<a id=\"run_1\"></a>\n## 1.6 Run the stream flow\n\n1. Click **Run**. \n1. If the flow does not start verify your stream flow. If no events are flowing from Message Hub operators make sure that your producer (simulating user activity), which you've launched in notebook 1, is running. \n1. Click on any operator to display throughput information.\n\n<img src= \"https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_flow_run.png\"></img>\n\nCongratulations! You just created a flow that ingests clickstream data from Message Hub, aggregates data and stores it in Redis storage.\n\nNext you will deploy a simple Node.js application that monitors the Redis database and visualizes the aggregated data in real-time.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"part2\"></a>\n\n***\n# Part 2: Visualizing streaming data in a real-time dashboard\n***\n\n## Table of contents\n2.1 [Setup](#setup)<br>\n2.2 [Install and deploy dashboard app to IBM Cloud](#install_deploy)<br>\n    2.2.1 [Install and deploy the dashboard app automatically](#install_auto)<br>\n    2.2.2 [Install and deploy the dashboard app manually](#install_manually)<br>\n    2.2.3 [Install the dashboard app locally](#install_locally)<br>\n    \n\n<a id=\"setup\"></a>\n## 2.1 Setup\n\nBefore you use the example code in this notebook, follow these setup steps:\n\n### Collect Redis connection information\n\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">IBM Cloud Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned **Compose for Redis** service and click on the service instance name.\n1. In the _Overview_ tab locate the HTTPS connection string\n```\nredis://admin:**PASSWORDX@**HOSTNAME**:**PORT**\n```\n\n1. Note your `HOSTNAME`, `PORT`, `PASSWORD` and `uri` information.\n\n**If you have successfully completed part 1 of this notebook skip the next section and proceed to section [Install and deploy dashboard app to IBM Cloud](#install_deploy).**\n\n### Simulate clickstream data\n             \nIf you have not completed the first part of this notebook you need to simulate the output of a streams flow.\n<br>\nIn the next cell replace `**uri**` with your Redis database's URI and then run the cell.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "# @hidden_cell\nredis_uri='**uri**'", "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "!pip install redis\nimport redis\nimport time\n\n# Connect to Redis\nr = redis.StrictRedis.from_url(redis_uri)\nprint 'Inserting aggregated dummy data into Redis '\ni = 0\nwhile True:\n    i = i + 1\n    # Insert dummy aggregated data values for demonstration purposes\n    print '.',\n    r.hset('funnel', 'basket_count', 3*i);\n    r.hset('funnel', 'basket_total', 2*i);\n    r.hset('funnel', 'checkout_count', i);\n    r.hset('funnel', 'checkout_total', i*75);\n    r.hset('funnel', 'login_count', 5*i);\n    time.sleep(2)       \n    if (i > 100):\n        print '\\nSimulation complete'\n        break;", "outputs": []}, {"source": "<a id=\"install_deploy\"></a>\n## 2.2 Install and deploy dashboard app to IBM Cloud\n\nYou can install the dashboard app and deploy it to IBM Cloud either automatically through the click of a button, manually or locally.\n\n<a id=\"install_auto\"></a>\n### 2.2.1 Install and deploy the dashboard app automatically\n\n**Note**: Automatic deployment will only succeed if your Compose for Redis service instance in IBM Cloud is named `ComposeForRedis-WDPBeta`.\n\nTo install the app automatically:\n\n1. Click the following button: <a target=\"_blank\" href=\"https://bluemix.net/deploy?repository=https://github.com/ibm-watson-data-lab/advo-beta-dashboard\">\n    <img src=\"http://bluemix.net/deploy/button.png\" alt=\"Deploy to IBM Cloud\"/>\n</a> <br/>\nThis button installs the Node.js app that will act as the real-time dashboard to visualize the streaming data. The code of this Node.js app is open-source and published on this  <a target=\"_blank\" href=\"https://github.com/ibm-watson-data-lab/advo-beta-dashboard\">GitHub repository</a>.\n1. In the _Deploy to Bluemix_ wizard select **betatest** as your space.\n<img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_deploy.png'></img>\n\n1. Click **Deploy** to deploy the app to IBM Cloud and follow the instructions to view the application.\n \n<img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_deploy_2.png'></img>\n\n> Note: The application deployment might take a couple of minutes. Once in the dashboard, any hyperlinks are placeholders. They are styled for example purposes only.\n\n<img src='https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/dynamic_analysis_dashboardapp.gif' width='1500'></img>\n\n<a id=\"install_manually\"></a>\n### 2.2.2 Install and deploy the dashboard app manually to IBM Cloud\n\nYou can choose to modify the app code yourself then install it and deploy it manually from the public GitHub repository:\n\n```sh\n# clone the code\ngit clone https://github.com/ibm-watson-data-lab/advo-beta-dashboard\n\n# change directory\ncd advo-beta-dashboard\n\n# deploy the dashboard to IBM Cloud\ncf push\n\n```\n\n**Note**: If your Compose for Redis service instance is not named `ComposeForRedis-WDPBeta` your deployment will fail with error _Could not find service ComposeForRedis-WDPBeta to bind to dashboard_. To resolve the issue, open `manifest.yml` and replace all occurrences of `ComposeForRedis-WDPBeta` with the name of your service instance name.\n\n\n<a id=\"install_locally\"></a>\n### 2.2.3 Install and run the dashboard app locally\n\nYou can choose to run the dashboard on your own machine with <a target=\"_blank\" href=\"https://nodejs.org/en/download/\">Node.js</a>[]() installed. To do so, add the Compose for Redis credentials and run the following commands:\n\n```sh\n# clone the code\ngit clone https://github.com/ibm-watson-data-lab/advo-beta-dashboard\n\n# change directory\ncd advo-beta-dashboard\n\n# install dependencies\nnpm install\n\n# TODO: Replace **HOSTNAME**, **PORT** and **PASSWORD** with your Redis credentials\nexport REDIS_URL=\"redis://x:**PASSWORD**@**HOSTNAME**:**PORT**\"\n\n# run the dashboard\nnpm start\n```\n\nAt the end of the setup, the dashboard app displays the port that the app is using, for example:\n\n```\nDashboard app listening on port 6039\n```\n\nSimply go to the http://localhost:6039 in your web browser to access the dashboard app and visualize the streaming data.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"summary\"></a>\n## Summary and next steps\n\nYou successfully completed this notebook! You learned how to create a streams flow to selectively aggregate data on the fly and visualize it in a Node.js dashboard web application.\n\nCheck out other notebooks in this series: \n - Localcart scenario two: Static data analysis using Python and PixieDust\n - Localcart scenario three: Build a product recommendation engine\n - Localcart scenario four: Build a revenue dashboard using PixieApps\n\nCopyright \u00a9 2017,2018 IBM. This notebook and its source code are released under the terms of the MIT License.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "source": "", "outputs": []}], "metadata": {"language_info": {"pygments_lexer": "ipython2", "version": "2.7.11", "codemirror_mode": {"version": 2, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.1", "name": "python2-spark21"}}, "nbformat_minor": 1}