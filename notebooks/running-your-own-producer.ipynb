{"nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython2", "name": "python", "version": "2.7.11", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}}, "kernelspec": {"language": "python", "name": "python2-spark20", "display_name": "Python 2 with Spark 2.0"}}, "nbformat": 4, "cells": [{"source": "# LocalCart: Running your own clickstream producer\n***\n\nThe clickstream producer simulates user activity for a ficticious web company. In the LocalCart scenario you'll be deriving insights from this activity using the streaming analytics service:\n<img src=\"https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/images/localcart_producer.png\"><img>\n\n\nIf you choose to run your own instance of the producer follow the instructions below.\n\n### 1.Provision a Message Hub service instance\n\n1. [Provision a Message Hub service instance in IBM Cloud.](https://console.bluemix.net/catalog/services/message-hub?taxonomyNavigation=apps)\n1. Open the _Service Credentials_ tab and view the credentials.\n1. Note the **user**, **password**, and **api_key** credentials.\n\n\n### 2. Load this notebook \n\n1. [In Watson Data Platform create a new project.](https://dataplatform.ibm.com/projects?context=analytics)\n1. Create a new notebook from URL https://raw.githubusercontent.com/ibm-watson-data-lab/localcart-at-index-conf/master/notebooks/running-your-own-producer.ipynb\n\n\n### 3. Load the clickstream producer application", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": "import pixiedust\npixiedust.installPackage(\"https://github.com/ibm-watson-data-lab/advo-beta-producer/raw/master/dist/LocalCartKafkaProducer-1.0-SNAPSHOT-uber.jar\")", "outputs": [], "cell_type": "code"}, {"source": "### 4. Set the Message Hub credentials\nReplace the `**USER**`, `**PASSWORD**`, and `**API_KEY**` placeholders in the cell below with the credentials from your setup.", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": "%%scala\nSystem.setProperty(\"KAFKA_USER_NAME\",\"**USER**\")\nSystem.setProperty(\"KAFKA_PASSWORD\",\"**PASSWORD**\")\nSystem.setProperty(\"KAFKA_API_KEY\",\"**API_KEY**\")\nSystem.setProperty(\"USE_JAAS\", \"true\")", "outputs": [], "cell_type": "code"}, {"source": "<a id=\"run_direct_step_3\"></a>\n### 5. Start the data stream", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": "%%scala\nimport com.ibm.localcart.DataStream\nprintln(DataStream.getInstance());", "outputs": [], "cell_type": "code"}, {"source": "### 6. Verify that events are flowing by looking at the stats\n> You can run this cell multiple times to make sure that events continue to be generated.", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": "%%scala\nimport com.ibm.localcart.DataStream\nprintln(DataStream.getInstance().getStats)", "outputs": [], "cell_type": "code"}]}